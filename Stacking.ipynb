{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03163e16176ae511bf06e7263d16ad4eb0b405b4",
    "collapsed": true
   },
   "source": [
    "This kernal is an attempt to use Stcking method for combining different models at multiple levels. The intent is to demostrate \n",
    "the stacking technique. The data preprocessing and feature engineering part is taken from \"Exploratory data analysis + LightGBM\"\n",
    "kernal by \"Gaxx\". \n",
    "\n",
    "I used LGB as base models and extra tree, Logistic and MLP models on 2nd and 3rd level stack. Feel free to add your own models. Comments and suggestions are welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "mycols = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\n",
    "sns.set_palette(palette = mycols, n_colors = 4)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../input/train.csv')\n",
    "test_set = pd.read_csv('../input/test.csv')\n",
    "\n",
    "#print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n",
    "#print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "20b51144c6ff9d8da7a3f0eb4df12587cbbc553b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.627394\n",
       "2    0.167103\n",
       "3    0.126504\n",
       "1    0.079000\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at target\n",
    "target = train_set['Target']\n",
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cac672fe0a74db69bed50e3f00cbaf0f0c2d6cd"
   },
   "source": [
    "## Outlier\n",
    "\n",
    "I had a look into train and test set, it turned out there is only one outlier to **rez_esc** in test set, and acorrding to the answer from competition host(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403), we can safely change the value to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e63c96ffa1a579326c997669f805f7437aa2b910"
   },
   "outputs": [],
   "source": [
    "#outlier in test set which rez_esc is 99.0\n",
    "test_set.loc[test_set['rez_esc'] == 99.0 , 'rez_esc'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91474a271cb33326deae7dd9c9ab51986b0d3762"
   },
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "cdf205203f38708c0c2895a5d3035b5b3d79557d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hogar_mayor</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idhogar</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Count\n",
       "Id                0\n",
       "hogar_mayor       0\n",
       "parentesco10      0\n",
       "parentesco11      0\n",
       "parentesco12      0\n",
       "idhogar           0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_na = train_set.isnull().sum().values / train_set.shape[0] *100\n",
    "df_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\n",
    "df_na = df_na.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "missing_value_count = df_na[df_na['Count']>0].shape[0]\n",
    "\n",
    "#print(f'We got {missing_value_count} rows which have missing value in train set ')\n",
    "df_na.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0faa5992610eb926fe8846828f4dde041e6ee0ae"
   },
   "source": [
    "*  **rez_esc** represents \"years behind in school\", missing value could be filled as 0\n",
    "*  **meaneduc** represents \"average years of education for adults (18+)\", missing value could be filled as 0\n",
    "*  **v18q1** really depends on v18q\n",
    "*  **v2a1** depends on tipovivi3\n",
    "\n",
    "We do not really need SQBxxxx features for polynomial in our case, and i will use fillna as 0 after at the last step of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cade5d48350aa95149c4d719b06bed2ac42cb9bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hogar_mayor</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentesco12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idhogar</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Count\n",
       "Id                0\n",
       "hogar_mayor       0\n",
       "parentesco10      0\n",
       "parentesco11      0\n",
       "parentesco12      0\n",
       "idhogar           0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_na = train_set.isnull().sum().values / train_set.shape[0] *100\n",
    "df_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\n",
    "df_na = df_na.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "missing_value_count = df_na[df_na['Count']>0].shape[0]\n",
    "\n",
    "#print(f'We got {missing_value_count} rows which have missing value in test set ')\n",
    "df_na.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "954c123321909d1c399f04c92d07dba193d6985a"
   },
   "outputs": [],
   "source": [
    "#Fill na\n",
    "def repalce_v18q1(x):\n",
    "    if x['v18q'] == 0:\n",
    "        return x['v18q']\n",
    "    else:\n",
    "        return x['v18q1']\n",
    "\n",
    "train_set['v18q1'] = train_set.apply(lambda x : repalce_v18q1(x),axis=1)\n",
    "test_set['v18q1'] = test_set.apply(lambda x : repalce_v18q1(x),axis=1)\n",
    "\n",
    "train_set['v2a1'] = train_set['v2a1'].fillna(value=train_set['tipovivi3'])\n",
    "test_set['v2a1'] = test_set['v2a1'].fillna(value=test_set['tipovivi3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e0b4f570715a4549790e7448f883303efcb0fd"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Replace object value, because some labels were generated whenever continuous variables have 1 or 0. The rule is to have being 1 yes=1 and no=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "baf15867d29fb88d76ae7a6d42dbe9ce81fcdcef"
   },
   "outputs": [],
   "source": [
    "cols = ['edjefe', 'edjefa']\n",
    "train_set[cols] = train_set[cols].replace({'no': 0, 'yes':1}).astype(float)\n",
    "test_set[cols] = test_set[cols].replace({'no': 0, 'yes':1}).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89e6aa2f9f8c5d04f65a0fc7adc40ec5aa0b13e9"
   },
   "source": [
    "It turns out orignial data lost one feature both for **roof** and **electricity**, so we manually add new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "96c687f372df2a40687ec0bfecb2c0bcfe252f9f"
   },
   "outputs": [],
   "source": [
    "train_set['roof_waste_material'] = np.nan\n",
    "test_set['roof_waste_material'] = np.nan\n",
    "train_set['electricity_other'] = np.nan\n",
    "test_set['electricity_other'] = np.nan\n",
    "\n",
    "def fill_roof_exception(x):\n",
    "    if (x['techozinc'] == 0) and (x['techoentrepiso'] == 0) and (x['techocane'] == 0) and (x['techootro'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def fill_no_electricity(x):\n",
    "    if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_set['roof_waste_material'] = train_set.apply(lambda x : fill_roof_exception(x),axis=1)\n",
    "test_set['roof_waste_material'] = test_set.apply(lambda x : fill_roof_exception(x),axis=1)\n",
    "train_set['electricity_other'] = train_set.apply(lambda x : fill_no_electricity(x),axis=1)\n",
    "test_set['electricity_other'] = test_set.apply(lambda x : fill_no_electricity(x),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35c334bf4514fbe2c0ba6ce5c5fb64c922cff7df",
    "collapsed": true
   },
   "source": [
    "More feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "394ec0f30fcf45e7f35fbb871f6c05868ef12e17"
   },
   "outputs": [],
   "source": [
    "train_set['adult'] = train_set['hogar_adul'] - train_set['hogar_mayor']\n",
    "train_set['dependency_count'] = train_set['hogar_nin'] + train_set['hogar_mayor']\n",
    "train_set['dependency'] = train_set['dependency_count'] / train_set['adult']\n",
    "train_set['child_percent'] = train_set['hogar_nin']/train_set['hogar_total']\n",
    "train_set['elder_percent'] = train_set['hogar_mayor']/train_set['hogar_total']\n",
    "train_set['adult_percent'] = train_set['hogar_adul']/train_set['hogar_total']\n",
    "test_set['adult'] = test_set['hogar_adul'] - test_set['hogar_mayor']\n",
    "test_set['dependency_count'] = test_set['hogar_nin'] + test_set['hogar_mayor']\n",
    "test_set['dependency'] = test_set['dependency_count'] / test_set['adult']\n",
    "test_set['child_percent'] = test_set['hogar_nin']/test_set['hogar_total']\n",
    "test_set['elder_percent'] = test_set['hogar_mayor']/test_set['hogar_total']\n",
    "test_set['adult_percent'] = test_set['hogar_adul']/test_set['hogar_total']\n",
    "\n",
    "train_set['rent_per_adult'] = train_set['v2a1']/train_set['hogar_adul']\n",
    "train_set['rent_per_person'] = train_set['v2a1']/train_set['hhsize']\n",
    "test_set['rent_per_adult'] = test_set['v2a1']/test_set['hogar_adul']\n",
    "test_set['rent_per_person'] = test_set['v2a1']/test_set['hhsize']\n",
    "\n",
    "train_set['overcrowding_room_and_bedroom'] = (train_set['hacdor'] + train_set['hacapo'])/2\n",
    "test_set['overcrowding_room_and_bedroom'] = (test_set['hacdor'] + test_set['hacapo'])/2\n",
    "\n",
    "train_set['no_appliances'] = train_set['refrig'] + train_set['computer'] + train_set['television']\n",
    "test_set['no_appliances'] = test_set['refrig'] + test_set['computer'] + test_set['television']\n",
    "\n",
    "train_set['r4h1_percent_in_male'] = train_set['r4h1'] / train_set['r4h3']\n",
    "train_set['r4m1_percent_in_female'] = train_set['r4m1'] / train_set['r4m3']\n",
    "train_set['r4h1_percent_in_total'] = train_set['r4h1'] / train_set['hhsize']\n",
    "train_set['r4m1_percent_in_total'] = train_set['r4m1'] / train_set['hhsize']\n",
    "train_set['r4t1_percent_in_total'] = train_set['r4t1'] / train_set['hhsize']\n",
    "test_set['r4h1_percent_in_male'] = test_set['r4h1'] / test_set['r4h3']\n",
    "test_set['r4m1_percent_in_female'] = test_set['r4m1'] / test_set['r4m3']\n",
    "test_set['r4h1_percent_in_total'] = test_set['r4h1'] / test_set['hhsize']\n",
    "test_set['r4m1_percent_in_total'] = test_set['r4m1'] / test_set['hhsize']\n",
    "test_set['r4t1_percent_in_total'] = test_set['r4t1'] / test_set['hhsize']\n",
    "\n",
    "train_set['rent_per_room'] = train_set['v2a1']/train_set['rooms']\n",
    "train_set['bedroom_per_room'] = train_set['bedrooms']/train_set['rooms']\n",
    "train_set['elder_per_room'] = train_set['hogar_mayor']/train_set['rooms']\n",
    "train_set['adults_per_room'] = train_set['adult']/train_set['rooms']\n",
    "train_set['child_per_room'] = train_set['hogar_nin']/train_set['rooms']\n",
    "train_set['male_per_room'] = train_set['r4h3']/train_set['rooms']\n",
    "train_set['female_per_room'] = train_set['r4m3']/train_set['rooms']\n",
    "train_set['room_per_person_household'] = train_set['hhsize']/train_set['rooms']\n",
    "\n",
    "test_set['rent_per_room'] = test_set['v2a1']/test_set['rooms']\n",
    "test_set['bedroom_per_room'] = test_set['bedrooms']/test_set['rooms']\n",
    "test_set['elder_per_room'] = test_set['hogar_mayor']/test_set['rooms']\n",
    "test_set['adults_per_room'] = test_set['adult']/test_set['rooms']\n",
    "test_set['child_per_room'] = test_set['hogar_nin']/test_set['rooms']\n",
    "test_set['male_per_room'] = test_set['r4h3']/test_set['rooms']\n",
    "test_set['female_per_room'] = test_set['r4m3']/test_set['rooms']\n",
    "test_set['room_per_person_household'] = test_set['hhsize']/test_set['rooms']\n",
    "\n",
    "train_set['rent_per_bedroom'] = train_set['v2a1']/train_set['bedrooms']\n",
    "train_set['edler_per_bedroom'] = train_set['hogar_mayor']/train_set['bedrooms']\n",
    "train_set['adults_per_bedroom'] = train_set['adult']/train_set['bedrooms']\n",
    "train_set['child_per_bedroom'] = train_set['hogar_nin']/train_set['bedrooms']\n",
    "train_set['male_per_bedroom'] = train_set['r4h3']/train_set['bedrooms']\n",
    "train_set['female_per_bedroom'] = train_set['r4m3']/train_set['bedrooms']\n",
    "train_set['bedrooms_per_person_household'] = train_set['hhsize']/train_set['bedrooms']\n",
    "\n",
    "test_set['rent_per_bedroom'] = test_set['v2a1']/test_set['bedrooms']\n",
    "test_set['edler_per_bedroom'] = test_set['hogar_mayor']/test_set['bedrooms']\n",
    "test_set['adults_per_bedroom'] = test_set['adult']/test_set['bedrooms']\n",
    "test_set['child_per_bedroom'] = test_set['hogar_nin']/test_set['bedrooms']\n",
    "test_set['male_per_bedroom'] = test_set['r4h3']/test_set['bedrooms']\n",
    "test_set['female_per_bedroom'] = test_set['r4m3']/test_set['bedrooms']\n",
    "test_set['bedrooms_per_person_household'] = test_set['hhsize']/test_set['bedrooms']\n",
    "\n",
    "train_set['tablet_per_person_household'] = train_set['v18q1']/train_set['hhsize']\n",
    "train_set['phone_per_person_household'] = train_set['qmobilephone']/train_set['hhsize']\n",
    "test_set['tablet_per_person_household'] = test_set['v18q1']/test_set['hhsize']\n",
    "test_set['phone_per_person_household'] = test_set['qmobilephone']/test_set['hhsize']\n",
    "\n",
    "train_set['age_12_19'] = train_set['hogar_nin'] - train_set['r4t1']\n",
    "test_set['age_12_19'] = test_set['hogar_nin'] - test_set['r4t1']    \n",
    "\n",
    "train_set['escolari_age'] = train_set['escolari']/train_set['age']\n",
    "test_set['escolari_age'] = test_set['escolari']/test_set['age']\n",
    "\n",
    "train_set['rez_esc_escolari'] = train_set['rez_esc']/train_set['escolari']\n",
    "train_set['rez_esc_r4t1'] = train_set['rez_esc']/train_set['r4t1']\n",
    "train_set['rez_esc_r4t2'] = train_set['rez_esc']/train_set['r4t2']\n",
    "train_set['rez_esc_r4t3'] = train_set['rez_esc']/train_set['r4t3']\n",
    "train_set['rez_esc_age'] = train_set['rez_esc']/train_set['age']\n",
    "test_set['rez_esc_escolari'] = test_set['rez_esc']/test_set['escolari']\n",
    "test_set['rez_esc_r4t1'] = test_set['rez_esc']/test_set['r4t1']\n",
    "test_set['rez_esc_r4t2'] = test_set['rez_esc']/test_set['r4t2']\n",
    "test_set['rez_esc_r4t3'] = test_set['rez_esc']/test_set['r4t3']\n",
    "test_set['rez_esc_age'] = test_set['rez_esc']/test_set['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "b32875cc4bb262c0abf8dbf3572a53b5de54a116"
   },
   "outputs": [],
   "source": [
    "train_set['dependency'] = train_set['dependency'].replace({np.inf: 0})\n",
    "test_set['dependency'] = test_set['dependency'].replace({np.inf: 0})\n",
    "\n",
    "#print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n",
    "#print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0de981d7bb9dab13b1986f51dd785126424de1c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "aggr_mean_list = ['rez_esc', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco2',\n",
    "             'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12',\n",
    "             'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',]\n",
    "\n",
    "other_list = ['escolari', 'age', 'escolari_age']\n",
    "\n",
    "for item in aggr_mean_list:\n",
    "    group_train_mean = train_set[item].groupby(train_set['idhogar']).mean()\n",
    "    group_test_mean = test_set[item].groupby(test_set['idhogar']).mean()\n",
    "    new_col = item + '_aggr_mean'\n",
    "    df_train[new_col] = group_train_mean\n",
    "    df_test[new_col] = group_test_mean\n",
    "\n",
    "for item in other_list:\n",
    "    for function in ['mean','std','min','max','sum']:\n",
    "        group_train = train_set[item].groupby(train_set['idhogar']).agg(function)\n",
    "        group_test = test_set[item].groupby(test_set['idhogar']).agg(function)\n",
    "        new_col = item + '_' + function\n",
    "        df_train[new_col] = group_train\n",
    "        df_test[new_col] = group_test\n",
    "\n",
    "#print(f'new aggregate train set has {df_train.shape[0]} rows, and {df_train.shape[1]} features')\n",
    "#print(f'new aggregate test set has {df_test.shape[0]} rows, and {df_test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "89a3bcb5a500b9a453df40f0d436cc1fb9b32513"
   },
   "outputs": [],
   "source": [
    "df_test = df_test.reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "\n",
    "train_agg = pd.merge(train_set, df_train, on='idhogar')\n",
    "test = pd.merge(test_set, df_test, on='idhogar')\n",
    "\n",
    "#fill all na as 0\n",
    "train_agg.fillna(value=0, inplace=True)\n",
    "test.fillna(value=0, inplace=True)\n",
    "#print(f'new train set has {train_agg.shape[0]} rows, and {train_agg.shape[1]} features')\n",
    "#print(f'new test set has {test.shape[0]} rows, and {test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "3f59d61995955c7cf2a82f277c2c9cbee82511d6"
   },
   "outputs": [],
   "source": [
    "#According to data descriptions,ONLY the heads of household are used in scoring. /\n",
    "#All household members are included in test + the sample submission, but only heads of households are scored.\n",
    "train = train_agg.query('parentesco1==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "84543b31c9523d0f262155d72cc727a54291a45a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "submission = test[['Id']]\n",
    "\n",
    "#Remove useless feature to reduce dimension\n",
    "train.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n",
    "test.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n",
    "\n",
    "correlation = train.corr()\n",
    "correlation = correlation['Target'].sort_values(ascending=False)\n",
    "#print(f'The most 20 positive feature: \\n{correlation.head(20)}')\n",
    "#print('*'*50)\n",
    "\n",
    "#print(f'The most 20 negative feature: \\n{correlation.tail(20)}')\n",
    "y = train['Target']\n",
    "\n",
    "train.drop(columns=['Target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        OOF_columns = []\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                S_train[test_idx, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]                \n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "            \n",
    "            #print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                #print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                #print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            #print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                #print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            #print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "\n",
    "            \n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM params\n",
    "lgb_params_1 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 750,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 500,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 1200,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.3,  \n",
    "    'num_leaves': 16,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 475,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base models\n",
    "lgb_model_1 = LGBMClassifier(**lgb_params_1)\n",
    "\n",
    "lgb_model_2 = LGBMClassifier(**lgb_params_2)\n",
    "\n",
    "lgb_model_3 = LGBMClassifier(**lgb_params_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacker models\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, max_depth=6, min_samples_split=10, random_state=10)\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier_1 fold 1\n",
      "Fit LGBMClassifier_1 fold 2\n",
      "Fit LGBMClassifier_1 fold 3\n",
      "Fit LGBMClassifier_2 fold 1\n",
      "Fit LGBMClassifier_2 fold 2\n",
      "Fit LGBMClassifier_2 fold 3\n",
      "Fit LGBMClassifier_3 fold 1\n",
      "Fit LGBMClassifier_3 fold 2\n",
      "Fit LGBMClassifier_3 fold 3\n",
      "\n",
      "\n",
      "Correlation between out-of-fold predictions from Base models:\n",
      "\n",
      "\n",
      "              Base model_1  Base model_2  Base model_3\n",
      "Base model_1      1.000000      0.760799           NaN\n",
      "Base model_2      0.760799      1.000000           NaN\n",
      "Base model_3           NaN           NaN           NaN\n",
      "\n",
      "\n",
      "Fit MLPClassifier_1 subset 1\n",
      "Fit MLPClassifier_1 subset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit MLPClassifier_1 subset 3\n",
      "Fit ExtraTreesClassifier_2 subset 1\n",
      "Fit ExtraTreesClassifier_2 subset 2\n",
      "Fit ExtraTreesClassifier_2 subset 3\n",
      "\n",
      "\n",
      "Correlation between without-one-column predictions from 1st level models:\n",
      "\n",
      "\n",
      "                   1st level model_1  1st level model_2\n",
      "1st level model_1           1.000000           0.917497\n",
      "1st level model_2           0.917497           1.000000\n",
      "\n",
      "\n",
      "Fit LogisticRegression for final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mode 2 run\n",
    "stack = Ensemble(mode=2,\n",
    "        n_splits=3,\n",
    "        stacker_2 = (log_model),         \n",
    "        stacker_1 = (mlp_model,et_model),\n",
    "        base_models = (lgb_model_1,lgb_model_2,lgb_model_3))       \n",
    "        \n",
    "y_pred = stack.fit_predict(train, y, test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission['Target'] = y_pred\n",
    "sample_submission.to_csv('Stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
